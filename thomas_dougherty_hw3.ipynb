{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Homework 3\n",
    "<p> Thomas Dougherty <br>\n",
    "9/28/2023 <br>\n",
    "Deep Learning</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./train/\n",
      "    Split: Train\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "\n",
    "# download training data\n",
    "train_data = CIFAR10(root=\"./train/\",\n",
    "                     train=True,\n",
    "                     download=True,\n",
    "                     transform=None)\n",
    "\n",
    "print(train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`How do you access the label?` <br>\n",
    "<p> The label can be accessed with with direct indexing of the [1] index of a train_data[i] tuple </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Label: 9\n",
      "Class: truck\n"
     ]
    }
   ],
   "source": [
    "label = train_data[16][1]\n",
    "data_class = train_data.classes[train_data[16][1]]\n",
    "print(f'Data Label: {label}')                       # print the label of the tuple\n",
    "print(f'Class: {data_class}')    # print the class of the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`What method is called when you index into a Dataset?`\n",
    "\n",
    "The Dataset function __getitem__(self, index: int) is called. It takes a self-reference and integer as arguments to return a tuple containing the image and target class.\n",
    "\n",
    "`Is CIFAR10 a class that is derived from the Dataset class?`\n",
    "\n",
    "Yes, CIFAR10 is a subclass of Dataset that inherits the __getitem__ and __len__ methods from Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Inheritance Tree`\n",
    "<p> do this</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-process the data before running through a neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data without transforms: <PIL.Image.Image image mode=RGB size=32x32 at 0x7FA62FCB61D0>\n",
      "Data Label: 9\n",
      "Training data with transforms: tensor([[[ 2.2039,  2.2427,  2.2621,  ..., -2.4291, -2.4291, -2.4291],\n",
      "         [ 2.2233,  2.3009,  2.3202,  ..., -2.4291, -2.4291, -2.4291],\n",
      "         [ 2.2621,  2.3202,  2.3590,  ..., -2.4291, -2.4291, -2.4291],\n",
      "         ...,\n",
      "         [ 0.2461,  0.1879,  0.1685,  ..., -2.4291, -2.4291, -2.4291],\n",
      "         [ 0.2461,  0.2073,  0.1491,  ..., -2.4291, -2.4291, -2.4291],\n",
      "         [-2.4291, -2.4291, -2.4291,  ..., -2.4291, -2.4291, -2.4291]],\n",
      "\n",
      "        [[ 2.3018,  2.3411,  2.3608,  ..., -2.4183, -2.4183, -2.4183],\n",
      "         [ 2.3215,  2.3805,  2.4198,  ..., -2.4183, -2.4183, -2.4183],\n",
      "         [ 2.3805,  2.4198,  2.4591,  ..., -2.4183, -2.4183, -2.4183],\n",
      "         ...,\n",
      "         [ 0.2564,  0.1974,  0.1778,  ..., -2.4183, -2.4183, -2.4183],\n",
      "         [ 0.2564,  0.2171,  0.1581,  ..., -2.4183, -2.4183, -2.4183],\n",
      "         [-2.4183, -2.4183, -2.4183,  ..., -2.4183, -2.4183, -2.4183]],\n",
      "\n",
      "        [[ 2.5001,  2.5391,  2.5586,  ..., -2.2214, -2.2214, -2.2214],\n",
      "         [ 2.5196,  2.5976,  2.6172,  ..., -2.2214, -2.2214, -2.2214],\n",
      "         [ 2.5586,  2.6172,  2.6367,  ..., -2.2214, -2.2214, -2.2214],\n",
      "         ...,\n",
      "         [ 0.4515,  0.3930,  0.3735,  ..., -2.2214, -2.2214, -2.2214],\n",
      "         [ 0.4515,  0.4125,  0.3540,  ..., -2.2214, -2.2214, -2.2214],\n",
      "         [-2.2214, -2.2214, -2.2214,  ..., -2.2214, -2.2214, -2.2214]]])\n",
      "Data Label: 9\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# TRAINING DATA\n",
    "\n",
    "# taking mean and std values from the book\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465),\n",
    "                        std=(0.2023, 0.1994, 0.201)),\n",
    "])\n",
    "\n",
    "train_data_xform = CIFAR10(root=\"./train/\",\n",
    "                     train=True,\n",
    "                     transform=train_transform)\n",
    "\n",
    "data, label = train_data[16]\n",
    "print(f'Training data without transforms: {data}')\n",
    "print(f'Data Label: {label}')\n",
    "\n",
    "data, label = train_data_xform[16]\n",
    "print(f'Training data with transforms: {data}')\n",
    "print(f'Data Label: {label}')\n",
    "\n",
    "\n",
    "# TESTING DATA\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        (0.4914, 0.4822, 0.4465),\n",
    "        (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "\n",
    "test_data = CIFAR10(root=\"./test/\",\n",
    "                     train=False,\n",
    "                     transform=test_transform,\n",
    "                     download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`When you instantiate train_data the second time, with the transform, try without download=True. Look at the API. What does it say?`\n",
    "<p>If download is set to 'True', the data is downloaded to the root directory, otherwise it will verify the data has been downloaded. If the 'download' option is not included and the Dataset is not found, it will cause a runtime error.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`What is the difference between training and testing transforms? Training is supposed to ”see” more data variability and that is why we provide augmentations of the original data through transforms. Why do you think the test dataset has a different transform?`\n",
    "\n",
    "The training transform has augmentations such as horizontal flips and random crops whereas the testing data is unaltered aside from normalization. Keeping the two sets seperated like this helps simulate real-world variabiltiy and the model will make generalizations about unseen data. Augmenting the testing data the same way as the training data may lead to overly-optimistic results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Please do` <br>\n",
    "`data, label = train_data[index] in both cases (with and without transforms).`<br>\n",
    "`Why is your result different when you apply transforms?`\n",
    "\n",
    "Before transforms are applied, the data is raw. When accessing train_data[index], metadata is returned such as the color mode, size, and address in memory. When the data is processed with a transform, it's changed into a tensor so that it is readable by a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Batching\n",
    "<p> I've noticed that bigger batch size results in faster training times, likely because it takes better advtange of GPU parallelism. However, it also leads to higher loss value. The larger batch size causes a smoother average since extreme or outlying datapoints have less influence over the loss value. The learning rate and the number of epochs has to be adjusted in order to get accurate results. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 3, 32, 32])\n",
      "torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "# Dataloader does all the work of shuffling data between batches and training cycles (epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data_xform,\n",
    "    batch_size = 512,           # Bigger batch size results in faster training times, and higher memory usage\n",
    "    shuffle = True)\n",
    "\n",
    "# create data batches\n",
    "data_batch, labels_batch = next(iter(train_loader))\n",
    "\n",
    "print(data_batch.size())\n",
    "print(labels_batch.size())\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    batch_size = 512,\n",
    "    shuffle = False) # set shuffle to false for the testing data for repeatable results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/thomas/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Layers: \n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vgg16 = models.vgg16(pretrained=True)     # faster convergence on pre-trained models\n",
    "\n",
    "# Print layers of the neural network\n",
    "print(\"Neural Network Layers: \")\n",
    "print(vgg16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the Linear transformation layer with a new definition\n",
    "# There are 10 classes in the CIFAR10 dataset, so 10 possible output features\n",
    "vgg16.classifier[-1] = nn.Linear(4096, 10)\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "model = vgg16.to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "<p> I spent too much time trying to debug the training loop when it kept crashing my laptop. Tried again on my desktop and it turned out my laptop just couldn't handle it The training loop took around 20-25 minutes to complete with the original parameters but I managed to get it down to around 6 minutes. It was fun playing around with this to get the loop to run in a more efficient way while trying to maintain around 80% accuracy </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting criterion...\n",
      "Implementing stochastic gradient descent...\n",
      "Starting epoch 0...\n",
      "Epoch: 0 Loss: 1.001998674504611\n",
      "Starting epoch 1...\n",
      "Epoch: 1 Loss: 0.5679168841060327\n",
      "Starting epoch 2...\n",
      "Epoch: 2 Loss: 0.4683391150771355\n",
      "Starting epoch 3...\n",
      "Epoch: 3 Loss: 0.4046045754637037\n",
      "Starting epoch 4...\n",
      "Epoch: 4 Loss: 0.3539463248179883\n",
      "Starting epoch 5...\n",
      "Epoch: 5 Loss: 0.33032100450019447\n",
      "Starting epoch 6...\n",
      "Epoch: 6 Loss: 0.2879910795968406\n",
      "Starting epoch 7...\n",
      "Epoch: 7 Loss: 0.2691903699721609\n"
     ]
    }
   ],
   "source": [
    "print(\"Setting criterion...\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"Implementing stochastic gradient descent...\")\n",
    "optimizer = optim.SGD(model.parameters(),\n",
    "                      lr=0.010, \n",
    "                      momentum=0.9)\n",
    "\n",
    "\n",
    "N_EPOCHS = 8\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(f'Starting epoch {epoch}...')\n",
    "    epoch_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device) \n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()   # forget errors of previous pass, let's start fresh\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)   # compute loss\n",
    "        \n",
    "        loss.backward()         # backpropegation; compute gradient\n",
    "        \n",
    "        optimizer.step()        # adjust parameters based on gradient \n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    print(\"Epoch: {} Loss: {}\".format(epoch, \n",
    "                  epoch_loss/len(train_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 512\n",
      "Test Accuracy: 0.8656250238418579\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0.0\n",
    "\n",
    "for x_test_batch, y_test_batch in test_loader:\n",
    "\n",
    "    model.eval()\n",
    "    y_test_batch = y_test_batch.to(device)\n",
    "    x_test_batch = x_test_batch.to(device)\n",
    "\n",
    "    y_pred_batch = model(x_test_batch)\n",
    "    _, predicted = torch.max(y_pred_batch, 1)\n",
    "    num_correct += (predicted == y_test_batch).float().sum()\n",
    "\n",
    "vgg_accuracy = num_correct/(len(test_loader)*test_loader.batch_size)\n",
    "\n",
    "print(len(test_loader), test_loader.batch_size)\n",
    "print(\"Test Accuracy: {}\".format(vgg_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR10 vs LeNet5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LeNet class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5) # <1>\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, int(x.nelement() / x.shape[0]))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "device = \"cuda\"\n",
    "model = LeNet5().to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LeNet training loop \n",
    "<p> Using same parameters as the VGG test </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting criterion...\n",
      "Implementing stochastic gradient descent...\n",
      "Starting epoch 0...\n",
      "Epoch: 0 Loss: 2.2525579953680235\n",
      "Starting epoch 1...\n",
      "Epoch: 1 Loss: 1.928471863269806\n",
      "Starting epoch 2...\n",
      "Epoch: 2 Loss: 1.7487856198330314\n",
      "Starting epoch 3...\n",
      "Epoch: 3 Loss: 1.6546363648103208\n",
      "Starting epoch 4...\n",
      "Epoch: 4 Loss: 1.5817009161929696\n",
      "Starting epoch 5...\n",
      "Epoch: 5 Loss: 1.534278276015301\n",
      "Starting epoch 6...\n",
      "Epoch: 6 Loss: 1.479981238744697\n",
      "Starting epoch 7...\n",
      "Epoch: 7 Loss: 1.4372382419449943\n"
     ]
    }
   ],
   "source": [
    "print(\"Setting criterion...\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"Implementing stochastic gradient descent...\")\n",
    "optimizer = optim.SGD(model.parameters(),\n",
    "                      lr=0.010, \n",
    "                      momentum=0.9)\n",
    "\n",
    "\n",
    "N_EPOCHS = 8\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(f'Starting epoch {epoch}...')\n",
    "    epoch_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device) \n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()   # forget errors of previous pass, let's start fresh\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)   # compute loss\n",
    "        \n",
    "        loss.backward()         # backpropegation; compute gradient\n",
    "        \n",
    "        optimizer.step()        # adjust parameters based on gradient \n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    print(\"Epoch: {} Loss: {}\".format(epoch, \n",
    "                  epoch_loss/len(train_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 512\n",
      "LeNet5 Test Accuracy: 0.525097668170929\n",
      "VGG16 Test Accuracy: 0.8656250238418579\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0.0\n",
    "\n",
    "for x_test_batch, y_test_batch in test_loader:\n",
    "\n",
    "    model.eval()\n",
    "    y_test_batch = y_test_batch.to(device)\n",
    "    x_test_batch = x_test_batch.to(device)\n",
    "\n",
    "    y_pred_batch = model(x_test_batch)\n",
    "    _, predicted = torch.max(y_pred_batch, 1)\n",
    "    num_correct += (predicted == y_test_batch).float().sum()\n",
    "\n",
    "lenet_accuracy = num_correct/(len(test_loader)*test_loader.batch_size)\n",
    "\n",
    "print(len(test_loader), test_loader.batch_size)\n",
    "print(\"LeNet5 Test Accuracy: {}\".format(lenet_accuracy))\n",
    "print(\"VGG16 Test Accuracy: {}\".format(vgg_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Please compare these two performances on CIFAR10. Why is one better than another?`\n",
    "<p> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
