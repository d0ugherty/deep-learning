{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "\n",
    "# download training data\n",
    "train_data = CIFAR10(root=\"./train/\",\n",
    "                     train=True,\n",
    "                     download=True,\n",
    "                     transform=None)\n",
    "\n",
    "print(train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`How do you access the label?` <br>\n",
    "<p> The label can be accessed with by getting the value of the [1] index of a train_data[i] tuple </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = train_data[16][1]\n",
    "data_class = train_data.classes[train_data[16][1]]\n",
    "print(f'Data Label: {label}')                       # print the label of the tuple\n",
    "print(f'Class: {data_class}')    # print the class of the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`What method is called when you index into a Dataset?`\n",
    "\n",
    "The Dataset function __getitem__(self, index: int) is called. It takes a self-reference and integer as arguments to return a tuple containing the image and target class.\n",
    "\n",
    "`Is CIFAR10 a class that is derived from the Dataset class?`\n",
    "\n",
    "Yes, CIFAR10 is a subclass of Dataset that inherits the __getitem__ and __len__ methods from Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Inheritance Tree`\n",
    "<p> do this</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-process the data before running through a neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING DATA\n",
    "\n",
    "# taking mean and std values from the book\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465),\n",
    "                        std=(0.2023, 0.1994, 0.201)),\n",
    "])\n",
    "\n",
    "train_data_xform = CIFAR10(root=\"./train/\",\n",
    "                     train=True,\n",
    "                     transform=train_transform)\n",
    "\n",
    "data, label = train_data[16]\n",
    "print(\"Training data without transforms: \")\n",
    "print(data)\n",
    "print(label)\n",
    "\n",
    "data, label = train_data_xform[16]\n",
    "print(\"Training data with transforms: \")\n",
    "print(data)\n",
    "print(label)\n",
    "\n",
    "\n",
    "\n",
    "# TESTING DATA\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        (0.4914, 0.4822, 0.4465),\n",
    "        (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "\n",
    "test_data = CIFAR10(root=\"./test/\",\n",
    "                     train=False,\n",
    "                     transform=test_transform,\n",
    "                     download=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`When you instantiate train_data the second time, with the transform, try without download=True. Look at the API. What does it say?`\n",
    "<p>If download is set to 'True', the data is downloaded to the root directory, otherwise it will verify the data has been downloaded. If the 'download' option is not included and the Dataset is not found, it will cause a runtime error.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`What is the difference between training and testing transforms? Training is supposed to ”see” more data variability and that is why we provide augmentations of the original data through transforms. Why do you think the test dataset has a different transform?`\n",
    "\n",
    "The training transform has augmentations such as horizontal flips and random crops whereas the testing data is unaltered aside from normalization. Keeping the two sets seperated like this helps simulate real-world variabiltiy and makes the make generalizations about unseen data. Augmenting the testing data may lead to overly-optimistic results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Please do` <br>\n",
    "`data, label = train_data[index] in both cases (with and without transforms).`<br>\n",
    "`Why is your result different when you apply transforms?`\n",
    "\n",
    "Before transforms are applied, the data is raw. When accessing train_data[index], metadata is returned such as the color mode, size, and address in memory. When the data is processed with a transform, it's changed into a tensor so that it is readable by a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader does all the work of shuffling data between batches and training cycles (epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data_xform,\n",
    "    batch_size = 16,\n",
    "    shuffle = True)\n",
    "\n",
    "# create data batches\n",
    "data_batch, labels_batch = next(iter(train_loader))\n",
    "print(data_batch.size())\n",
    "# out: torch.Size([16, 3, 32, 32])\n",
    "\n",
    "print(labels_batch.size())\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    batch_size = 16,\n",
    "    shuffle = False) # set shuffle to false for the testing data for repeatable results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = models.vgg16(pretrained=False)\n",
    "\n",
    "# Print layers of the neural network\n",
    "print(\"Neural Network Layers: \")\n",
    "print(vgg16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the Linear transformation layer with a new definition\n",
    "vgg16.classifier[-1] = nn.Linear(4096, 10)\n",
    "\n",
    "# not enough memory on laptop GPU\n",
    "device = \"cpu\"\n",
    "\n",
    "model = vgg16.to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Setting criterion...\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"Implementing stochastic gradient descent...\")\n",
    "optimizer = optim.SGD(model.parameters(),\n",
    "                      lr=0.001, \n",
    "                      momentum=0.9)\n",
    "\n",
    "N_EPOCHS = 10 \n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(epoch)\n",
    "    epoch_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device) \n",
    "        labels = labels.to(device)\n",
    "\n",
    "        #optimizer.zero_grad()   # forget errors of previous pass, let's start fresh\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)   # compute loss\n",
    "        \n",
    "        loss.backward()         # backpropegation; compute gradient\n",
    "        \n",
    "        optimizer.step()        # adjust parameters based on gradient \n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        print(epoch_loss)\n",
    "    print(\"Epoch: {} Loss: {}\".format(epoch, \n",
    "                  epoch_loss/len(train_loader)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
