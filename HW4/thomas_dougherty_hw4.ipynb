{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "<p> Thomas Dougherty <br>\n",
    "10-14-2023 </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`What is transfer learning? Look it up, and write how you understood it.`\n",
    "<p> Transfer learning is the act of training a model for one task (taking a pretrained model) and fine tuning it for another specific task. Transfer learning is a more efficient way of training neural networks because it allows them to use knowledge from the previous task while working with less labeled data. </br>\n",
    "\n",
    "It's like going from a riding a mountain bike to learning how to ride a motorized dirt bike. All the time spent on a mountain bike taught you balance, cornering technique, and riding over rough terrain. Since those previous skills don't have to br learned again, you can focus on things like throttle control and riding at high speed giving you a headstart on the learning process. </p> </br>\n",
    "\n",
    "`Explain where the differences are that make the loss plot different between the first two notebooks.`\n",
    "1. neaclass2 is using a pretrained resnet18 model whereas neaclass1 is using a newly instantiated resnet18 model. This gives it a head start in the training process.\n",
    "</br>\n",
    "2. neaclass2 has learning rate scheduler. By slowing incrementally slowing down the learning rate over a period of epochs, the model will find the simple patterns first then \"zero in\" on finding complex patterns.\n",
    "</br>\n",
    "\n",
    "`Finally, run neaclass3.ipynb  and explain the differences with respect to neoclass2.ipynb. At the end of neaclass3.ipynb create a test dataset from  NEUdata_split/Test and  use it to evaluate the accuracy of both its models.`\n",
    "1. neaclass3 deepcopies the original pre-trained resnet18 model.\n",
    "2. neaclass3 \n",
    "<br>\n",
    "Accuracy for original Resnet18 model came out to 95%, whereas accuracy for the vector model came out to around 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import DatasetUtils\n",
    "import torch\n",
    "from torch import optim \n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import DatasetFolder\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "### Design a convolutional autoencoder for a dataset of images 3 x 224 x224. Train it on NEU data for 50 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll start with getting the testing & training data and transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "19\n",
      "<torch.utils.data.dataset.Subset object at 0x7ffa5fcf2aa0>\n"
     ]
    }
   ],
   "source": [
    "def load_image(img_path:str):    \n",
    "        np_img = cv2.imread(img_path)\n",
    "        return Image.fromarray(np_img)\n",
    "\"\"\"\n",
    "test_loader = DatasetUtils.create_loader_and_transform(\n",
    "    root_path='RowanDLclassNEA/NEUdata_split/Test', \n",
    "    loader_func=load_image, \n",
    "    extensions=('.bmp',), \n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    is_test=True\n",
    ")\n",
    "\n",
    "train_loader, val_loader, train_set, val_set = DatasetUtils.create_loader_and_transform(\n",
    "    root_path='NEUdata', \n",
    "    loader_func=load_image, \n",
    "    extensions=('.bmp',), \n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    is_test=False\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "dset = DatasetFolder(root='NEUdata', loader = load_image, extensions = ('.bmp',))\n",
    "\n",
    "transform = transforms.Compose([\n",
    "  transforms.Resize(256),\n",
    "  transforms.CenterCrop(224),\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize(\n",
    "      mean=[0.485, 0.456, 0.406],\n",
    "      std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "test_xform = transforms.Compose([\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize(\n",
    "      mean=[0.485, 0.456, 0.406],\n",
    "      std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "dset = DatasetFolder(root='NEUdata', loader = load_image, extensions = ('.bmp',), transform = transform)\n",
    "test_data = DatasetFolder(root='RowanDLclassNEA/NEUdata_split/Test/', loader = load_image, extensions=('.bmp'), transform=test_xform)\n",
    "\n",
    "\n",
    "train_set, val_set = random_split(\n",
    "                      dset, \n",
    "                      [1200, 600])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                    train_set, \n",
    "                    batch_size=32, \n",
    "                    shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "                    val_set, \n",
    "                    batch_size=32, \n",
    "                    shuffle=True)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "            test_data,\n",
    "            batch_size = 32,\n",
    "            shuffle = False)\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(val_loader))\n",
    "#print(len(test_loader))\n",
    "print(train_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencorder Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found that pooling layers were needed for this just to bring the memory usage down to a reasonable level. I experimented with unpooling in the decoder but couldn't get it to work properly with the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "            Encoder Layers\n",
    "        \"\"\"\n",
    "        self.enc_conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.enc_pool1 = nn.MaxPool2d(2, stride=2, return_indices=True)\n",
    "\n",
    "        self.enc_conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.enc_pool2 = nn.MaxPool2d(2, stride=2, return_indices=True)\n",
    "\n",
    "        self.enc_conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.enc_pool3 = nn.MaxPool2d(2, stride=2, return_indices=True)\n",
    "\n",
    "        self.enc_conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.enc_pool4 = nn.MaxPool2d(2, stride=2, return_indices=True)\n",
    "\n",
    "        \"\"\"\n",
    "            Decoder Layers\n",
    "        \"\"\"\n",
    "        self.dec_unpool1 = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.dec_tconv1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "\n",
    "        self.dec_unpool2 = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.dec_tconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "\n",
    "        self.dec_unpool3 = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.dec_tconv3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "\n",
    "        self.dec_unpool4 = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.dec_tconv4 = nn.ConvTranspose2d(64, 3, kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            Encoder\n",
    "        \"\"\"\n",
    "        x = self.enc_conv1(x)\n",
    "        x, idx1 = self.enc_pool1(nn.ReLU()(x))\n",
    "\n",
    "        x = self.enc_conv2(x)\n",
    "        x, idx2 = self.enc_pool2(nn.ReLU()(x))\n",
    "\n",
    "        x = self.enc_conv3(x)\n",
    "        x, idx3 = self.enc_pool3(nn.ReLU()(x))\n",
    "\n",
    "        x = self.enc_conv4(x)\n",
    "        x, idx4 = self.enc_pool4(nn.ReLU()(x))\n",
    " \n",
    "        \"\"\" \n",
    "            Decoder \n",
    "        \"\"\"\n",
    "        x = self.dec_unpool1(self.dec_tconv1(nn.ReLU()(x)), idx4-1)\n",
    "        x = self.dec_unpool2(self.dec_tconv2(nn.ReLU()(x)), idx3)\n",
    "        x = self.dec_unpool3(self.dec_tconv3(nn.ReLU()(x)), idx2)\n",
    "        x = self.dec_unpool4(self.dec_tconv4(nn.ReLU()(x)), idx1)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate scheduler made a noticeable difference in bringing the training and value losses closer together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 392.00 MiB (GPU 0; 5.79 GiB total capacity; 3.98 GiB already allocated; 145.25 MiB free; 4.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/thomas/repos/deep-learning/HW4/thomas_dougherty_hw4.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/thomas/repos/deep-learning/HW4/thomas_dougherty_hw4.ipynb#X14sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/thomas/repos/deep-learning/HW4/thomas_dougherty_hw4.ipynb#X14sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/thomas/repos/deep-learning/HW4/thomas_dougherty_hw4.ipynb#X14sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m outputs \u001b[39m=\u001b[39m ae_model(images)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/thomas/repos/deep-learning/HW4/thomas_dougherty_hw4.ipynb#X14sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, images)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/thomas/repos/deep-learning/HW4/thomas_dougherty_hw4.ipynb#X14sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/thomas/repos/deep-learning/HW4/thomas_dougherty_hw4.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/thomas/repos/deep-learning/HW4/thomas_dougherty_hw4.ipynb#X14sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/thomas/repos/deep-learning/HW4/thomas_dougherty_hw4.ipynb#X14sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/thomas/repos/deep-learning/HW4/thomas_dougherty_hw4.ipynb#X14sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m        Encoder\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/thomas/repos/deep-learning/HW4/thomas_dougherty_hw4.ipynb#X14sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/thomas/repos/deep-learning/HW4/thomas_dougherty_hw4.ipynb#X14sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menc_conv1(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/thomas/repos/deep-learning/HW4/thomas_dougherty_hw4.ipynb#X14sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     x, idx1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menc_pool1(nn\u001b[39m.\u001b[39mReLU()(x))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/thomas/repos/deep-learning/HW4/thomas_dougherty_hw4.ipynb#X14sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menc_conv2(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 392.00 MiB (GPU 0; 5.79 GiB total capacity; 3.98 GiB already allocated; 145.25 MiB free; 4.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Initialize the model, loss, and optimizer\n",
    "ae_model = Autoencoder()\n",
    "device = device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ae_model = ae_model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()    \n",
    "optimizer = optim.SGD(ae_model.parameters(), \n",
    "                      lr=0.001, \n",
    "                      momentum=0.9)\n",
    "\n",
    "scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.25, total_iters=10)\n",
    "# Training loop for 50 epochs\n",
    "N_EPOCHS = 20\n",
    "tr_loss_hist = []\n",
    "val_loss_hist = []\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    # Training \n",
    "    train_loss = 0.0\n",
    "    ae_model.train() # <1>\n",
    "    for batch in train_loader:\n",
    "        images,_ = batch\n",
    "        images = images.cuda()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = ae_model(images)\n",
    "        loss = criterion(outputs, images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    val_loss = 0.0\n",
    "    ae_model.eval() # <2>\n",
    "    for batch in val_loader:\n",
    "        images,_ = batch\n",
    "        images = images.cuda()\n",
    "\n",
    "        outputs = ae_model(images)\n",
    "        loss = criterion(outputs, images)\n",
    "\n",
    "        val_loss += loss.item()\n",
    "\n",
    "    print(\"Epoch: {} Train Loss: {} Val Loss: {}\".format(\n",
    "                  epoch, \n",
    "                  train_loss/len(train_loader), \n",
    "                  val_loss/len(val_loader)))\n",
    "    tr_loss_hist.append(train_loss/len(train_loader))\n",
    "    val_loss_hist.append(val_loss/len(val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to store test losses\n",
    "total_mse_loss = 0.0\n",
    "total_mae_loss = 0.0\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "ae_model.eval()\n",
    "\n",
    "# Loop over test data\n",
    "for x_test_batch, _ in test_loader:  # Note: we don't need labels for autoencoders\n",
    "\n",
    "    # Move the test data to the same device as the model\n",
    "    x_test_batch = x_test_batch.to(device)\n",
    "\n",
    "    # Forward pass to get the model's reconstructed output\n",
    "    reconstructed_batch = ae_model(x_test_batch)\n",
    "    print(\"Shape of x_test_batch:\", x_test_batch.shape)\n",
    "    print(\"Shape of reconstructed_batch:\", reconstructed_batch.shape)\n",
    "\n",
    "    # Calculate the loss between the reconstructed and original data\n",
    "    mse_loss = nn.MSELoss()(reconstructed_batch, x_test_batch)\n",
    "    mae_loss = nn.L1Loss()(reconstructed_batch, x_test_batch)\n",
    "\n",
    "    # Accumulate the loss\n",
    "    total_mse_loss += mse_loss.item()\n",
    "    total_mae_loss += mae_loss.item()\n",
    "\n",
    "# Calculate the average loss\n",
    "average_mse_loss = total_mse_loss / len(test_loader)\n",
    "average_mae_loss = total_mae_loss / len(test_loader)\n",
    "\n",
    "print(f'Average MSE Loss: {average_mse_loss}')\n",
    "print(f'Average MAE Loss: {average_mae_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
