{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "<p> Thomas Dougherty <br>\n",
    "10-14-2023 </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`What is transfer learning? Look it up, and write how you understood it.`\n",
    "<p> Transfer learning is the act of training a model for one task (taking a pretrained model) and fine tuning it for another specific task. Transfer learning is a more efficient way of training neural networks because it allows them to use knowledge from the previous task while working with less labeled data. </br>\n",
    "\n",
    "It's like going from a riding a mountain bike to learning how to ride a motorized dirt bike. All the time spent on a mountain bike taught you balance, cornering technique, and riding over rough terrain. Since those previous skills don't have to br learned again, you can focus on things like throttle control and riding at high speed giving you a headstart on the learning process. </p> </br>\n",
    "\n",
    "`Explain where the differences are that make the loss plot different between the first two notebooks.`\n",
    "1. neaclass2 is using a pretrained resnet18 model whereas neaclass1 is using a newly instantiated resnet18 model. This gives it a head start in the training process.\n",
    "</br>\n",
    "2. neaclass2 has learning rate scheduler. By slowing incrementally slowing down the learning rate over a period of epochs, the model will find the simple patterns first then \"zero in\" on finding complex patterns.\n",
    "</br>\n",
    "\n",
    "`Finally, run neaclass3.ipynb  and explain the differences with respect to neoclass2.ipynb. At the end of neaclass3.ipynb create a test dataset from  NEUdata_split/Test and  use it to evaluate the accuracy of both its models.`\n",
    "1. neaclass3 deepcopies the original pre-trained resnet18 model.\n",
    "2. neaclass3 \n",
    "<br>\n",
    "Accuracy for original Resnet18 model came out to 95%, whereas accuracy for the vector model came out to around 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import DatasetUtils\n",
    "import torch\n",
    "from torch import optim \n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import DatasetFolder\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "### Design a convolutional autoencoder for a dataset of images 3 x 224 x224. Train it on NEU data for 50 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll start with getting the testing & training data and transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/thomas/repos/deep-learning/HW4/thomas_dougherty_hw4.ipynb Cell 7\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/thomas/repos/deep-learning/HW4/thomas_dougherty_hw4.ipynb#W4sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m train_loader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/thomas/repos/deep-learning/HW4/thomas_dougherty_hw4.ipynb#W4sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m                     train_set, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/thomas/repos/deep-learning/HW4/thomas_dougherty_hw4.ipynb#W4sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m                     batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/thomas/repos/deep-learning/HW4/thomas_dougherty_hw4.ipynb#W4sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m                     shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/thomas/repos/deep-learning/HW4/thomas_dougherty_hw4.ipynb#W4sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m val_loader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/thomas/repos/deep-learning/HW4/thomas_dougherty_hw4.ipynb#W4sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m                     val_set, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/thomas/repos/deep-learning/HW4/thomas_dougherty_hw4.ipynb#W4sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m                     batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/thomas/repos/deep-learning/HW4/thomas_dougherty_hw4.ipynb#W4sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m                     shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/thomas/repos/deep-learning/HW4/thomas_dougherty_hw4.ipynb#W4sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m test_loader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/thomas/repos/deep-learning/HW4/thomas_dougherty_hw4.ipynb#W4sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m             test_data,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/thomas/repos/deep-learning/HW4/thomas_dougherty_hw4.ipynb#W4sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m             batch_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/thomas/repos/deep-learning/HW4/thomas_dougherty_hw4.ipynb#W4sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m             shuffle \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/thomas/repos/deep-learning/HW4/thomas_dougherty_hw4.ipynb#W4sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(train_loader))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/thomas/repos/deep-learning/HW4/thomas_dougherty_hw4.ipynb#W4sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(val_loader))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "def load_image(img_path:str):    \n",
    "        np_img = cv2.imread(img_path)\n",
    "        return Image.fromarray(np_img)\n",
    "\"\"\"\n",
    "test_loader = DatasetUtils.create_loader_and_transform(\n",
    "    root_path='RowanDLclassNEA/NEUdata_split/Test', \n",
    "    loader_func=load_image, \n",
    "    extensions=('.bmp',), \n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    is_test=True\n",
    ")\n",
    "\n",
    "train_loader, val_loader, train_set, val_set = DatasetUtils.create_loader_and_transform(\n",
    "    root_path='NEUdata', \n",
    "    loader_func=load_image, \n",
    "    extensions=('.bmp',), \n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    is_test=False\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "dset = DatasetFolder(root='NEUdata', loader = load_image, extensions = ('.bmp',))\n",
    "\n",
    "transform = transforms.Compose([\n",
    "  transforms.Resize(256),\n",
    "  transforms.CenterCrop(224),\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize(\n",
    "      mean=[0.485, 0.456, 0.406],\n",
    "      std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "test_xform = transforms.Compose([\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize(\n",
    "      mean=[0.485, 0.456, 0.406],\n",
    "      std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "dset = DatasetFolder(root='NEUdata', loader = load_image, extensions = ('.bmp',), transform = transform)\n",
    "test_data = DatasetFolder(root='RowanDLclassNEA/NEUdata_split/Test/', loader = load_image, extensions=('.bmp'), transform=test_xform)\n",
    "\n",
    "\n",
    "train_set, val_set = random_split(\n",
    "                      dset, \n",
    "                      [1200, 600])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                    train_set, \n",
    "                    batch_size=32, \n",
    "                    shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "                    val_set, \n",
    "                    batch_size=32, \n",
    "                    shuffle=True)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "            test_data,\n",
    "            batch_size = 32,\n",
    "            shuffle = False)\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(val_loader))\n",
    "#print(len(test_loader))\n",
    "print(train_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencorder Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "    )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss, and optimizer\n",
    "ae_model = Autoencoder()\n",
    "device = device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ae_model = ae_model.to(device)\n",
    "\n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(ae_model.parameters(), \n",
    "                      lr=0.001, \n",
    "                      momentum=0.9)\n",
    "\n",
    "# Training loop for 50 epochs\n",
    "N_EPOCHS = 50\n",
    "tr_loss_hist = []\n",
    "val_loss_hist = []\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    # Training \n",
    "    train_loss = 0.0\n",
    "    ae_model.train() # <1>\n",
    "    for batch in train_loader:\n",
    "        images,_ = batch\n",
    "        images = images.cuda()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = ae_model(images)\n",
    "        loss = criterion(outputs, images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    val_loss = 0.0\n",
    "    ae_model.eval() # <2>\n",
    "    for batch in val_loader:\n",
    "        images,_ = batch\n",
    "        images = images.cuda()\n",
    "\n",
    "        outputs = ae_model(images)\n",
    "        loss = criterion(outputs, images)\n",
    "\n",
    "        val_loss += loss.item()\n",
    "\n",
    "    print(\"Epoch: {} Train Loss: {} Val Loss: {}\".format(\n",
    "                  epoch, \n",
    "                  train_loss/len(train_loader), \n",
    "                  val_loss/len(val_loader)))\n",
    "    tr_loss_hist.append(train_loss/len(train_loader))\n",
    "    val_loss_hist.append(val_loss/len(val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_correct = 0.0\n",
    "\n",
    "for x_test_batch, y_test_batch in test_loader:\n",
    "\n",
    "    ae_model.eval()\n",
    "    y_test_batch = y_test_batch.to(device)\n",
    "    x_test_batch = x_test_batch.to(device)\n",
    "\n",
    "    y_pred_batch = ae_model(x_test_batch)\n",
    "    _, predicted = torch.max(y_pred_batch, 1)\n",
    "    num_correct += (predicted == y_test_batch).float().sum()\n",
    "\n",
    "lenet_accuracy = num_correct/(len(test_loader)*test_loader.batch_size)\n",
    "\n",
    "print(len(test_loader), test_loader.batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
