{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "<p> Thomas Dougherty <br>\n",
    "10-14-2023 </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`What is transfer learning? Look it up, and write how you understood it.`\n",
    "<p> Transfer learning is the act of training a model for one task (taking a pretrained model) and fine tuning it for another specific task. Transfer learning is a more efficient way of training neural networks because it allows them to use knowledge from the previous task while working with less labeled data. </br>\n",
    "\n",
    "It's like going from a riding a mountain bike to learning how to ride a motorized dirt bike. All the time spent on a mountain bike taught you balance, cornering technique, and riding over rough terrain. Since those previous skills don't have to be learned again, you can focus on things like throttle control and riding at high speed giving you a headstart on the learning process. </p> </br>\n",
    "\n",
    "`Explain where the differences are that make the loss plot different between the first two notebooks.`\n",
    "1. neaclass2 is using a pretrained resnet18 model whereas neaclass1 is using a newly instantiated resnet18 model. This gives it a head start in the training process.\n",
    "</br>\n",
    "2. neaclass2 has learning rate scheduler. By slowing incrementally slowing down the learning rate over a period of epochs, the model will find the simple patterns first then \"zero in\" on finding complex patterns.\n",
    "</br>\n",
    "\n",
    "`Finally, run neaclass3.ipynb  and explain the differences with respect to neoclass2.ipynb. At the end of neaclass3.ipynb create a test dataset from  NEUdata_split/Test and  use it to evaluate the accuracy of both its models.`\n",
    "1. neaclass3 deepcopies the original pre-trained resnet18 model.\n",
    "2. neaclass3 \n",
    "<br>\n",
    "Accuracy for original Resnet18 model came out to 95%, whereas accuracy for the vector model came out to around 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'RowanDLclassNEA' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from utils import DatasetUtils\n",
    "import torch\n",
    "from torch import optim \n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import DatasetFolder\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "!git clone https://github.com/skokalj/RowanDLclassNEA.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "### Design a convolutional autoencoder for a dataset of images 3 x 224 x224. Train it on NEU data for 50 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll start with getting the testing & training data and transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "38\n",
      "<torch.utils.data.dataset.Subset object at 0x7f11b432dab0>\n"
     ]
    }
   ],
   "source": [
    "def load_image(img_path:str):    \n",
    "        np_img = cv2.imread(img_path)\n",
    "        return Image.fromarray(np_img)\n",
    "\"\"\"\n",
    "test_loader = DatasetUtils.create_loader_and_transform(\n",
    "    root_path='RowanDLclassNEA/NEUdata_split/Test', \n",
    "    loader_func=load_image, \n",
    "    extensions=('.bmp',), \n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    is_test=True\n",
    ")\n",
    "\n",
    "train_loader, val_loader, train_set, val_set = DatasetUtils.create_loader_and_transform(\n",
    "    root_path='RowanDLclassNEA/NEUdata', \n",
    "    loader_func=load_image, \n",
    "    extensions=('.bmp',), \n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    is_test=False\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "dset = DatasetFolder(root='RowanDLclassNEA/NEUdata', loader = load_image, extensions = ('.bmp',))\n",
    "\n",
    "transform = transforms.Compose([\n",
    "  transforms.Resize(256),\n",
    "  transforms.CenterCrop(224),\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize(\n",
    "      mean=[0.485, 0.456, 0.406],\n",
    "      std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "test_xform = transforms.Compose([\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize(\n",
    "      mean=[0.485, 0.456, 0.406],\n",
    "      std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "dset = DatasetFolder(root='RowanDLclassNEA/NEUdata', loader = load_image, extensions = ('.bmp',), transform = transform)\n",
    "test_data = DatasetFolder(root='RowanDLclassNEA/NEUdata_split/Test', loader = load_image, extensions=('.bmp'), transform=test_xform)\n",
    "\n",
    "\n",
    "train_set, val_set = random_split(\n",
    "                      dset, \n",
    "                      [1200, 600])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                    train_set, \n",
    "                    batch_size=16, \n",
    "                    shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "                    val_set, \n",
    "                    batch_size=16, \n",
    "                    shuffle=True)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "            test_data,\n",
    "            batch_size = 16,\n",
    "            shuffle = False)\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(val_loader))\n",
    "#print(len(test_loader))\n",
    "print(train_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencorder Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found that pooling layers were needed for this just to bring the memory usage down to a reasonable level. I experimented with unpooling in the decoder but couldn't get it to work properly with the training loop. <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "            Encoder:\n",
    "                Standard 3x3 kernels on the convolutional layers for feature extraction\n",
    "                2x2 kernels on the pooling layers for halving the input and reducing dimensions\n",
    "        \"\"\"\n",
    "        self.encoder = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),  \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            \n",
    "            # bad things happen beyond this point\n",
    "            #nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.ReLU(inplace=True),\n",
    "            #nn.MaxPool2d(2, stride=2),\n",
    "        )\n",
    "        \n",
    "        \"\"\"\n",
    "            Decoder:\n",
    "                Convolutional Transpose with kernel size 2 will double the size of the input\n",
    "        \"\"\"\n",
    "        self.decoder = nn.Sequential(\n",
    "           \n",
    "            #nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n",
    "            #nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"Input shape:\", x.shape)\n",
    "        x = self.encoder(x)\n",
    "        #print(\"Shape after encoding:\", x.shape)\n",
    "        x = self.decoder(x)\n",
    "        #print(\"Shape after decoding:\", x.shape)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate scheduler made a noticeable difference in bringing the training and value losses closer together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss: 0.9586588120460511 Val Loss: 0.9132498455675024\n",
      "Epoch: 1 Train Loss: 0.9568982179959615 Val Loss: 0.9058983435756282\n",
      "Epoch: 2 Train Loss: 0.9477023386955261 Val Loss: 0.8972044547921733\n",
      "Epoch: 3 Train Loss: 0.9203547978401184 Val Loss: 0.8383950983223162\n",
      "Epoch: 4 Train Loss: 0.6558027780055999 Val Loss: 0.38357642567471456\n",
      "Epoch: 5 Train Loss: 0.3356733371814092 Val Loss: 0.29914411941641256\n",
      "Epoch: 6 Train Loss: 0.3110330009460449 Val Loss: 0.29470371474560936\n",
      "Epoch: 7 Train Loss: 0.30916272819042206 Val Loss: 0.2956234091206601\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model, loss, and optimizer\n",
    "ae_model = Autoencoder()\n",
    "device = device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ae_model = ae_model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()    \n",
    "optimizer = optim.SGD(ae_model.parameters(), \n",
    "                      lr=0.010, \n",
    "                      momentum=0.9)\n",
    "\n",
    "scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.25, total_iters=10)\n",
    "# Training loop for 50 epochs\n",
    "N_EPOCHS = 50\n",
    "tr_loss_hist = []\n",
    "val_loss_hist = []\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    # Training \n",
    "    train_loss = 0.0\n",
    "    ae_model.train()\n",
    "    for batch in train_loader:\n",
    "        images,_ = batch\n",
    "        images = images.cuda()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = ae_model(images)\n",
    "        loss = criterion(outputs, images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    val_loss = 0.0\n",
    "    ae_model.eval()\n",
    "    for batch in val_loader:\n",
    "        images,_ = batch\n",
    "        images = images.cuda()\n",
    "\n",
    "        outputs = ae_model(images)\n",
    "        loss = criterion(outputs, images)\n",
    "\n",
    "        val_loss += loss.item()\n",
    "\n",
    "    print(\"Epoch: {} Train Loss: {} Val Loss: {}\".format(\n",
    "                  epoch, \n",
    "                  train_loss/len(train_loader), \n",
    "                  val_loss/len(val_loader)))\n",
    "    tr_loss_hist.append(train_loss/len(train_loader))\n",
    "    val_loss_hist.append(val_loss/len(val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "<p> Instead of testing the autoencoder on classification, we'll test the data loss between encoding and decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mse_loss = 0.0\n",
    "total_L1_loss = 0.0\n",
    "\n",
    "ae_model.eval()\n",
    "\n",
    "for x_test_batch, _ in test_loader:  \n",
    "\n",
    "    x_test_batch = x_test_batch.to(device)\n",
    "\n",
    "    reconstructed_batch = ae_model(x_test_batch)\n",
    "    \n",
    "    mse_loss = nn.MSELoss()(reconstructed_batch, x_test_batch)\n",
    "    L1_loss = nn.L1Loss()(reconstructed_batch, x_test_batch)\n",
    "\n",
    "    total_mse_loss += mse_loss.item()\n",
    "    total_L1_loss += L1_loss.item()\n",
    "\n",
    "# Calculate the average loss\n",
    "average_mse_loss = total_mse_loss / len(test_loader)\n",
    "average_L1_loss = total_L1_loss / len(test_loader)\n",
    "\n",
    "print(f'Average MSE Loss: {average_mse_loss}')\n",
    "print(f'Average L1 Loss: {average_L1_loss}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
