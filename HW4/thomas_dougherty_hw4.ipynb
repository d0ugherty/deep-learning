{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "<p> Thomas Dougherty <br>\n",
    "10-14-2023 </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`What is transfer learning? Look it up, and write how you understood it.`\n",
    "<p> Transfer learning is the act of training a model for one task (taking a pretrained model) and fine tuning it for another specific task. Transfer learning is a more efficient way of training neural networks because it allows them to use knowledge from the previous task while working with less labeled data. </br>\n",
    "\n",
    "It's like going from a riding a mountain bike to learning how to ride a motorized dirt bike. All the time spent on a mountain bike taught you balance, cornering technique, and riding over rough terrain. Since those previous skills don't have to br learned again, you can focus on things like throttle control and riding at high speed giving you a headstart on the learning process. </p> </br>\n",
    "\n",
    "`Explain where the differences are that make the loss plot different between the first two notebooks.`\n",
    "1. neaclass2 is using a pretrained resnet18 model whereas neaclass1 is using a newly instantiated resnet18 model. This gives it a head start in the training process.\n",
    "</br>\n",
    "2. neaclass2 has learning rate scheduler. By slowing incrementally slowing down the learning rate over a period of epochs, the model will find the simple patterns first then \"zero in\" on finding complex patterns.\n",
    "</br>\n",
    "\n",
    "`Finally, run neaclass3.ipynb  and explain the differences with respect to neoclass2.ipynb. At the end of neaclass3.ipynb create a test dataset from  NEUdata_split/Test and  use it to evaluate the accuracy of both its models.`\n",
    "1. neaclass3 deepcopies the original pre-trained resnet18 model.\n",
    "2. neaclass3 \n",
    "<br>\n",
    "Accuracy for original Resnet18 model came out to 95%, whereas accuracy for the vector model came out to around 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'RowanDLclassNEA' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#from utils import DatasetUtils\n",
    "import torch\n",
    "from torch import optim \n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import DatasetFolder\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "!git clone https://github.com/skokalj/RowanDLclassNEA.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "### Design a convolutional autoencoder for a dataset of images 3 x 224 x224. Train it on NEU data for 50 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll start with getting the testing & training data and transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "38\n",
      "<torch.utils.data.dataset.Subset object at 0x7f335dfa2400>\n"
     ]
    }
   ],
   "source": [
    "def load_image(img_path:str):    \n",
    "        np_img = cv2.imread(img_path)\n",
    "        return Image.fromarray(np_img)\n",
    "\"\"\"\n",
    "test_loader = DatasetUtils.create_loader_and_transform(\n",
    "    root_path='RowanDLclassNEA/NEUdata_split/Test', \n",
    "    loader_func=load_image, \n",
    "    extensions=('.bmp',), \n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    is_test=True\n",
    ")\n",
    "\n",
    "train_loader, val_loader, train_set, val_set = DatasetUtils.create_loader_and_transform(\n",
    "    root_path='NEUdata', \n",
    "    loader_func=load_image, \n",
    "    extensions=('.bmp',), \n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    is_test=False\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "dset = DatasetFolder(root='RowanDLclassNEA/NEUdata', loader = load_image, extensions = ('.bmp',))\n",
    "\n",
    "transform = transforms.Compose([\n",
    "  transforms.Resize(256),\n",
    "  transforms.CenterCrop(224),\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize(\n",
    "      mean=[0.485, 0.456, 0.406],\n",
    "      std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "test_xform = transforms.Compose([\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize(\n",
    "      mean=[0.485, 0.456, 0.406],\n",
    "      std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "dset = DatasetFolder(root='RowanDLclassNEA/NEUdata', loader = load_image, extensions = ('.bmp',), transform = transform)\n",
    "test_data = DatasetFolder(root='RowanDLclassNEA/NEUdata_split/Test', loader = load_image, extensions=('.bmp'), transform=test_xform)\n",
    "\n",
    "\n",
    "train_set, val_set = random_split(\n",
    "                      dset, \n",
    "                      [1200, 600])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                    train_set, \n",
    "                    batch_size=16, \n",
    "                    shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "                    val_set, \n",
    "                    batch_size=16, \n",
    "                    shuffle=True)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "            test_data,\n",
    "            batch_size = 16,\n",
    "            shuffle = False)\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(val_loader))\n",
    "#print(len(test_loader))\n",
    "print(train_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencorder Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found that pooling layers were needed for this just to bring the memory usage down to a reasonable level. I experimented with unpooling in the decoder but couldn't get it to work properly with the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"Input shape:\", x.shape)\n",
    "        x = self.encoder(x)\n",
    "        #print(\"Shape after encoding:\", x.shape)\n",
    "        x = self.decoder(x)\n",
    "        #print(\"Shape after decoding:\", x.shape)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate scheduler made a noticeable difference in bringing the training and value losses closer together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss: 0.8791889500617981 Val Loss: 0.8912959585064336\n",
      "Epoch: 1 Train Loss: 0.8661818257967631 Val Loss: 0.8717702282102484\n",
      "Epoch: 2 Train Loss: 0.8097550594806671 Val Loss: 0.6910032065291154\n",
      "Epoch: 3 Train Loss: 0.5286659320195516 Val Loss: 0.4192412362286919\n",
      "Epoch: 4 Train Loss: 0.38121739943822225 Val Loss: 0.33711929187962886\n",
      "Epoch: 5 Train Loss: 0.3266071234146754 Val Loss: 0.3005913496017456\n",
      "Epoch: 6 Train Loss: 0.2990867958466212 Val Loss: 0.2818088194257335\n",
      "Epoch: 7 Train Loss: 0.28451534390449523 Val Loss: 0.2718357408517285\n",
      "Epoch: 8 Train Loss: 0.27982172310352327 Val Loss: 0.26706628168099805\n",
      "Epoch: 9 Train Loss: 0.27839208314816155 Val Loss: 0.2653751157616314\n",
      "Epoch: 10 Train Loss: 0.27756725907325747 Val Loss: 0.2654864844915114\n",
      "Epoch: 11 Train Loss: 0.2770154458284378 Val Loss: 0.2653449918878706\n",
      "Epoch: 12 Train Loss: 0.2765861852963765 Val Loss: 0.2658187115662976\n",
      "Epoch: 13 Train Loss: 0.27623687585194906 Val Loss: 0.2657736598661071\n",
      "Epoch: 14 Train Loss: 0.2758258836468061 Val Loss: 0.2651831558660457\n",
      "Epoch: 15 Train Loss: 0.27564672867457074 Val Loss: 0.26608796064790924\n",
      "Epoch: 16 Train Loss: 0.27531450748443603 Val Loss: 0.26394481447182205\n",
      "Epoch: 17 Train Loss: 0.2750574964284897 Val Loss: 0.26286865280647026\n",
      "Epoch: 18 Train Loss: 0.27478931228319803 Val Loss: 0.2661836900209126\n",
      "Epoch: 19 Train Loss: 0.2745141144593557 Val Loss: 0.26415082028037623\n",
      "Epoch: 20 Train Loss: 0.2742928735415141 Val Loss: 0.26629340844719035\n",
      "Epoch: 21 Train Loss: 0.2739814958969752 Val Loss: 0.2640743998712615\n",
      "Epoch: 22 Train Loss: 0.27373487850030265 Val Loss: 0.26510620626964065\n",
      "Epoch: 23 Train Loss: 0.27348524987697603 Val Loss: 0.26266058632417727\n",
      "Epoch: 24 Train Loss: 0.2731569955746333 Val Loss: 0.2637465243276797\n",
      "Epoch: 25 Train Loss: 0.27291214565436045 Val Loss: 0.26101389448893697\n",
      "Epoch: 26 Train Loss: 0.27273332118988036 Val Loss: 0.26317699665301725\n",
      "Epoch: 27 Train Loss: 0.27235264658927916 Val Loss: 0.2605888594530131\n",
      "Epoch: 28 Train Loss: 0.2720641936858495 Val Loss: 0.25969521073918594\n",
      "Epoch: 29 Train Loss: 0.2717895834644635 Val Loss: 0.2627437993099815\n",
      "Epoch: 30 Train Loss: 0.27146115799744924 Val Loss: 0.2600123013712858\n",
      "Epoch: 31 Train Loss: 0.2712086028854052 Val Loss: 0.26075181933610064\n",
      "Epoch: 32 Train Loss: 0.2708986610174179 Val Loss: 0.25915339745973287\n",
      "Epoch: 33 Train Loss: 0.270635295410951 Val Loss: 0.2596839533040398\n",
      "Epoch: 34 Train Loss: 0.27032703042030337 Val Loss: 0.25970018204105527\n",
      "Epoch: 35 Train Loss: 0.27004042446613313 Val Loss: 0.2601829547631113\n",
      "Epoch: 36 Train Loss: 0.2697441534201304 Val Loss: 0.2586790168363797\n",
      "Epoch: 37 Train Loss: 0.26944600184758505 Val Loss: 0.258704590170007\n",
      "Epoch: 38 Train Loss: 0.26917215118805565 Val Loss: 0.2598959897693835\n",
      "Epoch: 39 Train Loss: 0.2688646864891052 Val Loss: 0.2614038343492307\n",
      "Epoch: 40 Train Loss: 0.2685632105668386 Val Loss: 0.25926288531014796\n",
      "Epoch: 41 Train Loss: 0.26828166246414187 Val Loss: 0.26013191005117015\n",
      "Epoch: 42 Train Loss: 0.2680312208334605 Val Loss: 0.25752180304966477\n",
      "Epoch: 43 Train Loss: 0.26770518491665524 Val Loss: 0.257370639788477\n",
      "Epoch: 44 Train Loss: 0.2674536826213201 Val Loss: 0.25650482546342046\n",
      "Epoch: 45 Train Loss: 0.26715054551760353 Val Loss: 0.2568788946067032\n",
      "Epoch: 46 Train Loss: 0.26687773952881494 Val Loss: 0.2568455742377984\n",
      "Epoch: 47 Train Loss: 0.2665844492117564 Val Loss: 0.25500612568698433\n",
      "Epoch: 48 Train Loss: 0.2663070835669835 Val Loss: 0.2543223568875539\n",
      "Epoch: 49 Train Loss: 0.26606386065483095 Val Loss: 0.2581146118280135\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model, loss, and optimizer\n",
    "ae_model = Autoencoder()\n",
    "device = device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ae_model = ae_model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()    \n",
    "optimizer = optim.SGD(ae_model.parameters(), \n",
    "                      lr=0.010, \n",
    "                      momentum=0.9)\n",
    "\n",
    "scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.25, total_iters=10)\n",
    "# Training loop for 50 epochs\n",
    "N_EPOCHS = 50\n",
    "tr_loss_hist = []\n",
    "val_loss_hist = []\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    # Training \n",
    "    train_loss = 0.0\n",
    "    ae_model.train() # <1>\n",
    "    for batch in train_loader:\n",
    "        images,_ = batch\n",
    "        images = images.cuda()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = ae_model(images)\n",
    "        loss = criterion(outputs, images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    val_loss = 0.0\n",
    "    ae_model.eval() # <2>\n",
    "    for batch in val_loader:\n",
    "        images,_ = batch\n",
    "        images = images.cuda()\n",
    "\n",
    "        outputs = ae_model(images)\n",
    "        loss = criterion(outputs, images)\n",
    "\n",
    "        val_loss += loss.item()\n",
    "\n",
    "    print(\"Epoch: {} Train Loss: {} Val Loss: {}\".format(\n",
    "                  epoch, \n",
    "                  train_loss/len(train_loader), \n",
    "                  val_loss/len(val_loader)))\n",
    "    tr_loss_hist.append(train_loss/len(train_loader))\n",
    "    val_loss_hist.append(val_loss/len(val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_test_batch: torch.Size([16, 3, 200, 200])\n",
      "Shape of reconstructed_batch: torch.Size([16, 3, 200, 200])\n",
      "Shape of x_test_batch: torch.Size([16, 3, 200, 200])\n",
      "Shape of reconstructed_batch: torch.Size([16, 3, 200, 200])\n",
      "Shape of x_test_batch: torch.Size([16, 3, 200, 200])\n",
      "Shape of reconstructed_batch: torch.Size([16, 3, 200, 200])\n",
      "Shape of x_test_batch: torch.Size([16, 3, 200, 200])\n",
      "Shape of reconstructed_batch: torch.Size([16, 3, 200, 200])\n",
      "Shape of x_test_batch: torch.Size([16, 3, 200, 200])\n",
      "Shape of reconstructed_batch: torch.Size([16, 3, 200, 200])\n",
      "Shape of x_test_batch: torch.Size([16, 3, 200, 200])\n",
      "Shape of reconstructed_batch: torch.Size([16, 3, 200, 200])\n",
      "Shape of x_test_batch: torch.Size([16, 3, 200, 200])\n",
      "Shape of reconstructed_batch: torch.Size([16, 3, 200, 200])\n",
      "Shape of x_test_batch: torch.Size([16, 3, 200, 200])\n",
      "Shape of reconstructed_batch: torch.Size([16, 3, 200, 200])\n",
      "Shape of x_test_batch: torch.Size([16, 3, 200, 200])\n",
      "Shape of reconstructed_batch: torch.Size([16, 3, 200, 200])\n",
      "Shape of x_test_batch: torch.Size([16, 3, 200, 200])\n",
      "Shape of reconstructed_batch: torch.Size([16, 3, 200, 200])\n",
      "Shape of x_test_batch: torch.Size([16, 3, 200, 200])\n",
      "Shape of reconstructed_batch: torch.Size([16, 3, 200, 200])\n",
      "Shape of x_test_batch: torch.Size([4, 3, 200, 200])\n",
      "Shape of reconstructed_batch: torch.Size([4, 3, 200, 200])\n",
      "Average MSE Loss: 0.18947125226259232\n",
      "Average MAE Loss: 0.3067781813442707\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables to store test losses\n",
    "total_mse_loss = 0.0\n",
    "total_mae_loss = 0.0\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "ae_model.eval()\n",
    "\n",
    "# Loop over test data\n",
    "for x_test_batch, _ in test_loader:  # Note: we don't need labels for autoencoders\n",
    "\n",
    "    # Move the test data to the same device as the model\n",
    "    x_test_batch = x_test_batch.to(device)\n",
    "\n",
    "    # Forward pass to get the model's reconstructed output\n",
    "    reconstructed_batch = ae_model(x_test_batch)\n",
    "    print(\"Shape of x_test_batch:\", x_test_batch.shape)\n",
    "    print(\"Shape of reconstructed_batch:\", reconstructed_batch.shape)\n",
    "\n",
    "    # Calculate the loss between the reconstructed and original data\n",
    "    mse_loss = nn.MSELoss()(reconstructed_batch, x_test_batch)\n",
    "    mae_loss = nn.L1Loss()(reconstructed_batch, x_test_batch)\n",
    "\n",
    "    # Accumulate the loss\n",
    "    total_mse_loss += mse_loss.item()\n",
    "    total_mae_loss += mae_loss.item()\n",
    "\n",
    "# Calculate the average loss\n",
    "average_mse_loss = total_mse_loss / len(test_loader)\n",
    "average_mae_loss = total_mae_loss / len(test_loader)\n",
    "\n",
    "print(f'Average MSE Loss: {average_mse_loss}')\n",
    "print(f'Average MAE Loss: {average_mae_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
