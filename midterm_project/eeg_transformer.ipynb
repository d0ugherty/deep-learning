{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy.signal as signal\n",
    "import mne\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from mne import preprocessing, Epochs\n",
    "import utils\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe from CSV files\n",
    "eeg_file_path = 'data/eeg_data_A/'\n",
    "eog_file_path = 'data/eeg_data_B/'\n",
    "#eeg_dataframe_A.head()\n",
    "#eeg_dataframe_A = utils.format_df(eeg_dataframe_A)\n",
    "#eeg_dataframe_A.head()\n",
    "#eeg_dataframe = utils.gdf_to_df(file_path + 'A01T.gdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Band-pass filtering (noise reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /run/media/thomas/hdd/repos/deep-learning/midterm_project/data/eeg_data_A/A09E.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Could not determine channel type of the following channels, they will be set as EEG:\n",
      "EEG-Fz, EEG, EEG, EEG, EEG, EEG, EEG, EEG-C3, EEG, EEG-Cz, EEG, EEG-C4, EEG, EEG, EEG, EEG, EEG, EEG, EEG, EEG-Pz, EEG, EEG, EOG-left, EOG-central, EOG-right\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 675097  =      0.000 ...  2700.388 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.11/contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  next(self.gen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/eeg_data_A/A09E.gdf\n",
      "Used Annotations descriptions: ['1023', '1072', '276', '277', '32766', '768', '783']\n",
      "Multiple event values for single event times found. Creating new event value to reflect simultaneous events.\n",
      "Not setting metadata\n",
      "585 matching events found\n",
      "Setting baseline interval to [-0.2, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 585 events and 176 original time points ...\n",
      "1 bad epochs dropped\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'eog_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/run/media/thomas/hdd/repos/deep-learning/midterm_project/eeg_transformer.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/thomas/hdd/repos/deep-learning/midterm_project/eeg_transformer.ipynb#W4sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     tmin, tmax \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m0.2\u001b[39m, \u001b[39m0.5\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/thomas/hdd/repos/deep-learning/midterm_project/eeg_transformer.ipynb#W4sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     eeg_epochs \u001b[39m=\u001b[39m mne\u001b[39m.\u001b[39mEpochs(raw, events, event_repeated\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmerge\u001b[39m\u001b[39m'\u001b[39m, event_id\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, tmin\u001b[39m=\u001b[39mtmin, tmax\u001b[39m=\u001b[39mtmax, baseline\u001b[39m=\u001b[39m(\u001b[39mNone\u001b[39;00m, \u001b[39m0\u001b[39m), preload\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/run/media/thomas/hdd/repos/deep-learning/midterm_project/eeg_transformer.ipynb#W4sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     eeg_epoch_objects\u001b[39m.\u001b[39mappend(eog_epochs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/thomas/hdd/repos/deep-learning/midterm_project/eeg_transformer.ipynb#W4sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     raw_eeg_objects\u001b[39m.\u001b[39mappend(raw)\n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/thomas/hdd/repos/deep-learning/midterm_project/eeg_transformer.ipynb#W4sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m raw\u001b[39m.\u001b[39mfilter(l_freq\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m, h_freq\u001b[39m=\u001b[39m\u001b[39m40.0\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'eog_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "eeg_files = glob.glob(os.path.join(eeg_file_path, '*.gdf'))\n",
    "eog_files = glob.glob(os.path.join(eog_file_path + '*.gdf'))\n",
    "eeg_channels = 25\n",
    "eog_channels = 6\n",
    "method = 'fastica'\n",
    "raw_eeg_objects = []  # list to store raw objects\n",
    "raw_eog_objects = []\n",
    "\n",
    "eog_epoch_objects = []\n",
    "eeg_epoch_objects = []\n",
    "rename_dict = {\n",
    "    'EOG:ch01': 'EOG001',\n",
    "    'EOG:ch02': 'EOG002',\n",
    "    'EOG:ch03': 'EOG003'\n",
    "}\n",
    "\n",
    "\n",
    "for file in eeg_files + eog_files:\n",
    "    raw = mne.io.read_raw_gdf(file, preload=True)\n",
    "    if file in eog_files:\n",
    "        print(file)\n",
    "        # Rename EOG channels and set channel types\n",
    "        raw.rename_channels(rename_dict)\n",
    "        raw.set_channel_types({'EOG001': 'eog', 'EOG002': 'eog', 'EOG003': 'eog'})\n",
    "\n",
    "        # Create EOG epochs\n",
    "        eog_epochs = mne.preprocessing.create_eog_epochs(raw, baseline=(-0.5, -0.2))\n",
    "        eog_epoch_objects.append(eog_epochs)\n",
    "        raw_eog_objects.append(raw)\n",
    "    else:\n",
    "        print(file)\n",
    "        events, event_id = mne.events_from_annotations(raw)\n",
    "        tmin, tmax = -0.2, 0.5\n",
    "        eeg_epochs = mne.Epochs(raw, events, event_repeated='merge', event_id=None, tmin=tmin, tmax=tmax, baseline=(None, 0), preload=True)\n",
    "        eeg_epoch_objects.append(eeg_epochs)\n",
    "        raw_eeg_objects.append(raw)\n",
    "    raw.filter(l_freq=1.0, h_freq=40.0, verbose=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to PyTorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18, 25, 687792])\n",
      "torch.Size([40, 6, 787729])\n"
     ]
    }
   ],
   "source": [
    "eeg_arrays = []\n",
    "eog_arrays = []\n",
    "\n",
    "for raw in raw_eeg_objects:\n",
    "    data = raw.get_data()\n",
    "    eeg_arrays.append(data)\n",
    "\n",
    "for raw in raw_eog_objects:\n",
    "    data = raw.get_data()\n",
    "    eog_arrays.append(data)\n",
    "\n",
    "eeg_arrays = utils.pad_arrays(eeg_arrays)\n",
    "eog_arrays = utils.pad_arrays(eog_arrays)\n",
    "\n",
    "tensor_eeg = torch.tensor(eeg_arrays,dtype=torch.float32)\n",
    "tensor_eog = torch.tensor(eog_arrays,dtype=torch.float32)\n",
    "\n",
    "print(tensor_eeg.shape)\n",
    "print(tensor_eog.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
